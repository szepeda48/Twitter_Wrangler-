{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Reporting for this Project\n",
    "Create a 300-600 word written report called wrangle_report.pdf or wrangle_report.html that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this poject I was tasked to look into three data set that needed to be gathered, merged and cleaned. Two of these report were provided for us while the other were had to attain using an API. This was a Twitter API and I used the Twitter IDs from the twitter_archive-enhanced.csv to looks for the tweets on the APi. There was lot of missing data and even some instances where the API was not able to locate some of the twitter_ids. There were some that had 0 favorit counts which I thought was very odd considering how well liked the WeRateDogs page on twitter is. \n",
    "\n",
    "During the assessing process i noticed there was a lot to be fixed: \n",
    "\n",
    "# Quality issues: \n",
    "\n",
    "\n",
    "\n",
    "1.Quaity: Drop Column we dont need:\n",
    "\n",
    "-There were a few columns that were not relevent to my assesmnet such as retweeted_status_id that did not give any valuable insight.\n",
    "\n",
    "\n",
    "2.Quality : Fix denominator\n",
    "\n",
    " All of the denominators are suppose to be 10 yet some were variously different numbers. \n",
    "\n",
    "\n",
    "3.Quality: Viewing and drop outliers \n",
    "\n",
    "For the rating numerator there were plenty of ratings that went beyond 10 but that was part of the fun on the twitter page so i only tried to remove outliers above 100 because they seemed far too exagerated. \n",
    "\n",
    "\n",
    "4.Quality Convert P1-P3 to string\n",
    "\n",
    "felt is was eaier to hand the value if they were string values. \n",
    "\n",
    "\n",
    "5.Quality: if there is a missing confidence , replace with 0 to make easier to compare to the other two.\n",
    "\n",
    "There were some confidence numbers that were missing and so to make it easier to read i just turned them into 0\n",
    "\n",
    "\n",
    "6.QUALITY : Clear all false names and convert to Nan\n",
    "\n",
    "There was some data that rated possible predictions(3) and choose the best prediction, therefore i deleted the predictions that were likly since i can not speculate and just need best guess on the breed of dog. \n",
    "\n",
    "\n",
    "7.Quality issue: Some of the name are capatolized and some are not\n",
    "\n",
    "This could cause fuuture issue if one wanted to catergorize the breeds in a graph \n",
    "\n",
    "\n",
    "8.Quality issue: Change Timestape to to_datetime\n",
    "\n",
    "Allows one to analyse the dates more effectivly \n",
    "\n",
    "\n",
    "9.Quality: Drop nan values in rows since we wont know wha dog they are rating or talking about.\n",
    "\n",
    "There some rows that did not have any dog predictions therefor we woud not be able to tell which dog was even liked. \n",
    "\n",
    "\n",
    "# Tidiness issues: \n",
    "\n",
    "\n",
    "\n",
    "1.Tidy Issue : Meerge alld dataframes to a master table\n",
    "\n",
    "-I had to merge all the data based on the twitter_id \n",
    "\n",
    "\n",
    "2.Tidy Issue , Clear all false name and combine 3 columns to give the bread and confidence only \n",
    "\n",
    "-has to combine the prediction do we only have the best guess prediction to work with since the other would be irrelevent. \n",
    "\n",
    "\n",
    "3.Tidy issue: Combine the stage column into one dog stage column \n",
    "\n",
    "-There were three seprate columns for the stages and I had to combine the to have one column to work with\n",
    "\n",
    "\n",
    "4.Tidy issue: Drop unneeded columns\n",
    "\n",
    "more columns needed to be dropped as the table became simpler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: It becomes a huge skill to wrangle and asses data because often times the data that you wrangle will more often then not be very dirty data. It will have the most Tidiness issue along with the most Quality issues. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
